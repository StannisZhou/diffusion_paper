\documentclass{article}
\begin{document}

\hfill{}\begin{minipage}{4in}
\raggedleft
Michael Thoennessen, Editor in Chief\\
CC: Terre Arena, Senior Editorial Assistant\\
Physical Review E\\
\end{minipage}

\vspace{.5in}

\noindent
Michael Thoennessen,

\vspace{.2in}

Thank you for the opportunity to revise our manuscript, {\em Capacities and the free passage of entropic barriers}. We appreciate the careful reviews and the constructive suggestions made by the two reviewers.   The revision addresses the points raised by the reviewers. We have attached an overview of the revisions, along with a point-by-point response to each of the reviewer comments. \\

\noindent
Sincerely,

Jackson Loper, Guangyao Zhou, and Stuart Geman\\\\


\newpage

\section{Revision overview}

Over the past few months we have performed a series of additional simulations and rewritten the article from scratch to closely follow the pointers from reviewer 2.

\begin{enumerate}
    \item We rewrote the introduction to:
    \begin{itemize}
        \item clarify what problems this method is good for,
        \item clarify how this work relates to existing literature,
        \item make our terminology more consistent with standard use.
    \end{itemize}
    \item We rewrote the preliminaries to more actively highlight the four key objects (reversible diffusions, hitting probabilities, $\varepsilon$-flatness, and capacities).
    \item We rewrote the main results section to trim out unnecessary mathematical details and focus on the key results.
    \item We rewrote the shell-method section to better highlight the main ideas. 
    \item We conducted more comprehensive numerical simulations of the technique's accuracy.  In the previous draft we used the numerical results mostly to verify the theory.  The new draft tries to take a more methodical look, using a wider variety of problems and quantifying the approximation error in a wider variety of ways.  
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Point-by-point}

\newcommand{\rev}[3]{\noindent\begin{tabular}{|p{4.5in}}\emph{Reviewer #1.}  #2\end{tabular}\vspace{.1in}

#3\vspace{.3in}}

\rev{1}{This paper is a mathematical theoretical study...}{

This paper does contain a fair amount of theory, but it also contains a computational aspect which was easy to miss in the first draft.  In fact, in our view this work centers around a new algorithm for estimating hitting probabilities in reversible diffusions.  To make this clear, we now devote the second paragraph of the introduction to focusing more clearly on the computational problem we're interested in.  We also clarify that this paper includes a practical algorithm for tackling this computational problem.}

\rev{1}{...that is not easy to follow for the non-mathematician}{

In the introduction of the new revision we have made the main ideas clearer:
\begin{itemize}
    \item Reversible diffusions -- in the first paragraph of the introduction we take a moment to actually describe reversible diffusions, giving a few examples from the sciences.  We defer the technical definition to the second section.
    \item Hitting probabilities -- in the second paragraph of the introduction we define these and give an example to clarify.  In the second section we give the technical definition.
    \item When does this algorithm apply -- Here we had a bit of difficulty, because there are two ways of thinking about this:
    \begin{itemize}
        \item On the one hand, there is a technical condition we actually use ($\varepsilon$-flatness).  The algorithm is only guaranteed to work if that condition holds
        \item On the other hand, we have a more the intuitive and vague condition which implies the technical condition. 
    \end{itemize} 
    In the third paragraph we try to explain both in progressive order from simple to sophisticated, so that the casual reader can skip unnecessary details.  In the second section we give a technical definition.
\end{itemize}
} 

\rev{1}{(also because of the notation that is not common for theoretical chemists/physicists).}{
    
We made a review of the literature and changed any notation/language that seemed non-standard.   If there is anything specific we should change we would of course be happy to do so.}

\rev{1}{References are not order and superscript citations should follow after
the punctuation. Last Eq on p6 (not numbered): I think it should be $\rho(x)$ and  not $\rho(dx)$}{
Fixed.
}

\rev{2}{The theory well done and the proofs are
carried out (and presented) in an understandable manner, but I failed
to recognize an immediate relevance for the proposed algorithm ... The authors might therefore reconsider taking some
stress from MD and focusing it more on mesoscopic bio-systems where
the algorithm could have a much higher applicability. }{

Following Reviewer 2's suggestion of looking at mesoscopic systems we now include a description of the ``narrow escape problem'' in the introduction.  This is at least one area of study where these methods should find immediately application.  For narrow escape problems, the hitting probabilities are proportional to the hitting times, so these methods can be applied to determine the relative hitting times to all kinds of different odd-shaped targets that that literature is interested in.  

}

\rev{2}{I
could not judge on the novelty of the proofs and also did not check
the existing literature, but some aspects did seem vaguely familiar
yet were unreferenced.}{

Our main results (Theorem 1 and 2) are substantially new to the literature, as far as we know.

Our work does contain many propositions and lemmas, and some of these are certainly known by at least some people.  For example, the results about capacity and flux follows quite quickly from Green's first identity applied to the relevant equations.  We also include an inequality about Bessel functions that is doubtless obvious to those that specialize in such things.  Unfortunately, we had a difficult time finding sources in the literature which proved exactly the things we needed.  For this reason we were unable to source many of the proofs, even though they are indeed similar to what is already published in the literature.     
}

\rev{2}{(i) The motivation builds heavily on the analysis of molecular dynamics data yet the algorithm will presumably be rather useless for the analysis of macromolecular systems because the central assumption
(of small absorbing points in a mostly flat landscape) is mostly incompatible with the respective landscapes.}{

We shifted the motivation to the narrow escape problem (which does have implications in mesoscopic bio-systems) for which the proposed algorithm is more obviously relevant.

However, we do still include some mention of macromolecular systems.  We note TCB McLeish has said that ``folding rates are controlled by the rate of diffusion of pieces of the open coil in their search for favorable contacts'' and ``the vast majority of the space covered by the energy landscape must actually be flat.''   There is also some experimental evidence that the exploration of regions with golf-course potentials is the rate-limiting step for a variety of processes. 

Some references we looked at that gave us this impression:

\begin{itemize}
    \item T C B McLeish. Protein folding in High-Dimensional spaces: Hypergutters and the role of nonnative interactions. Biophys. J., 88(1):172–183, January 2005.
    \item J M Goldberg and R L Baldwin. A specific transition state for s-peptide combining with folded s-protein and then refolding. Proc. Natl. Acad. Sci. U. S. A., 96(5):2019–2024, March 1999
    \item M Jacob, M Geeves, G Holtermann, and F X Schmid. Diffusional barrier crossing in a two-state protein folding reaction. Nat. Struct. Biol., 6(10):923–926, October 1999.
    \item Kevin W Plaxco and David Baker. Limited internal friction in the rate-limiting step of a two-state protein folding reaction. Proc. Natl. Acad. Sci. U. S. A., 95(23):13591–13596, November 1998.
    \item David J Wales. Energy landscapes: calculating pathways and rates. Int. Rev. Phys. Chem., 25(1-2):237–282, January 2006
\end{itemize}

If these results bear out, the first-passage time out of this exploration may be long and our conditions may hold for macromolecular diffusions. 

}

\rev{2}{...But with
the view of what is to be achieved algorithmically, I could not detect
a breakthrough with respect to what was done by Schuss, Keller or
Ward, partly decades ago...In particular, the golf-course
problem (as the authors called it) has been solved asymptotically by
singular perturbation theory and matched asymptotics.}{

For several special cases, Reviewer 2 is correct that there are existing methods which could be used to get reasonably accurate estimates for hitting probabilities.  We now include a section comparing our work with these asymptotic methods. 

In brief, there are three differences between this work and the approach taken by Schuss/Keller/Ward:
\begin{enumerate}
    \item Not all of our results are asymptotic.  One of our results is, but one of our results is not:
    \begin{itemize}
        \item Our main theorem really isn't asymptotic at all: it proposes a method for approximating hitting probabilities, and shows that the error of this approximation is bounded in terms of a value $\varepsilon$ which quantifies how much the hitting probabilities vary inside a set $M$.  
        \item We also have a second theorem which shows that this $\varepsilon$ vanishes as the target sets become smaller.  This is indeed an asymptotic result, although qualitatively different from the kinds of results found in the narrow escape literature.  Instead of showing how hitting times can be well-approximated (as the narrow escape literature does), this second theorem shows cases where that hitting probabilities are nearly constant inside a large region $M$.
    \end{itemize}

    \item Second, taking a step back, the the proposed algorithm is posed to handle a much wider variety of problems, because it requires very little special analysis to deal with the particular shape of the target.  By reducing the problem to a small region around each target, the proposed algorithm makes it feasible to compute the relevant properties numerically; no asymptotic analysis of the shape are necessary.  Moreover, the proposed method applies even in the case of nontrivial energy landscapes.  The axons studied in the narrow escape literature certainly contain large-scale fields, and this method should be able to handle those kinds of issues easily, whereas existing work focuses almost exclusively on the Brownian Motion case.

    \item Third, our method says \emph{nothing} about the overall timescale of the process -- this is a weakness of the algorithm proposed here.  The existing asymptotic analysis are able to give information about these timescale because they make strong assumptions about the process (e.g. that it is a Brownian motion in a sphere).  The algorithm proposed here is much more general, but it comes at a steep cost: we can't get timing information.
\end{enumerate}
We try to make these points clear in the new draft of the introduction.  The last point we also revisit in the discussion.
}

\rev{2}{(iii) the terminology is non-standard, occasionally unclear and often
even contrasting the standard use. For example, the term entropic
barrier...It was also not
entirely clear whether capacity is used in the same context as in
probabilistic potential theory}{

We tried to tighten this up as much as possible.  For example,
\begin{itemize}
    \item We removed all reference to ``entropic barriers.'' 
    \item What we used to call ``capacity'' we now call ``first passage capacity'' to differentiate it from all the other notions of set capacity.    We compare it with Harmonic capacity and cite the work of Anton Bovier (who uses essentially the same notion of capacity).  These capacities are indeed to be understood in the context of probabilistic potential theory, a fact we now mention.  
    \item We mention that our ``hitting probabilities'' are sometimes called ``splitting probabilities'' (especially in the case when there are exactly two targets; this is the case we mostly discuss though the algorithm holds equally well for any finite number of targets). 
\end{itemize}
We are certainly open to any further suggestions in this regard.
}

\rev{2}{The existing literature is extremely poorly described.}{

We now devote half of the introduction to describing what is already known.  We describe four different related literatures and relate our approach to each one.  

}

\rev{2}{However, the assumption of MSM is actually orthogonal to the
assumptions made in the present work (yet this appears to have slipped
under the radar). Namely, MSM are built on the assumption that the
residence time in the metastable cores (and hence the escape from a
core) is much longer than the time spent outside of the cores...splitting probabilities from a point located
initially *not* inside a stable core seem to bear little relevance in
describing conformational dynamics...Splitting probabilities in that sense (even if
determined with perfect accuracy) as a whole would e relevant for the
dynamics in-between the cores only upon the further assumption of
renewal dynamics between the cores.}{

In the previous draft we dove into many details about how the ideas motivating MSM might connect to the current proposed work, and this reviewer comment reflects many of those subtleties.  

However, in the interest of keeping the issues clear for a broad audience, the new draft has a much more limited focus.  In the new manuscript we focus on only two points; we believe that the authors and reviewer 2 already agree on these points:

\begin{itemize}
    \item MSM is different from our approach.  The set of situations where MSM can be applied is different from the set of situations where our approach can be applied.  Each approach requires a different set of assumptions to work.  As reviewer 2 points out, the empirical success of MSM suggests that its assumptions may indeed hold in a variety of cases.  The assumptions for first-passage capacities certainly hold in the narrow-escape setting and they may also hold in other situations.  In the introduction of the new draft we make these distinctions clear.

    \item If the relevant assumptions hold, the method proposed here could be used to achieve something \emph{similar} to what MSM techniques achieve.  Under the right assumptions, first-passage capacities could be used to accurately simulate the order in which a process explores a set of targets (specifically it suffices that every path between the targets passes through a set $M$ where the hitting probabilities are $\varepsilon$-flat).  We discuss this possibility at the end of the discussion of the new draft.
\end{itemize}

}

\rev{2}{I failed to see any quantitative evidence for the quality of the
approximations, as advertised in section 3. Nor did I could recognize
the gauge on what 'good agreement' is supposed to mean.}{
    
We have completely replaced the numerical results section, using six new experiments intended to more methodically assess the quality of the approximations.  We try to clearly investigate all possible sources of error in a capacity-based approach:
\begin{itemize}
    \item The hitting probability functions are not perfectly flat.  We investigate clearly exactly how flat they are in various circumstances.
    \item The hitting probability is not exactly equal to the ratio of capacities.  We investigate the error here, and find that it is significantly smaller than what might be expected from the results of Theorem 1.
    \item The capacity cannot be computed exactly.  Where the capacity can be computed exactly we evaluate the error of the shell method.  Where it cannot be computed exactly we evaluate the quality of the shell method by proxy, looking at the accuracy of the hitting probability estimates derived from the capacity estimates.  
\end{itemize}
We also now include two ways to think about the magnitude of these errors (to gauge what `good agreement' might mean).  First, we compare against the number of direct simulations necessary to achieve comparable accuracy (under the optimistic assumption that the direct simulations are perfectly exact).  Second, we compare the magnitude of the second two types of errors against the first; indeed, if the hitting probabilities vary by initial condition by $\varepsilon$ inside a region, then capacity-based methods (which give only a single estimate) cannot hope to get less error than $\varepsilon/2$.  This provides a kind of lower-bound we can measure against.



}
\end{document}